{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBkcat\n",
    "\n",
    "## 背景介绍：\n",
    "催化常数kcat（catalytic number）:是在底物浓度处于饱和状态下，一个酶（或一个酶活性位点）单位时间内转化的底物分子数，其定义了反应的最大化学转化率，值为Vmax/E。\n",
    "\n",
    "查尔姆斯理工大学（Chalmers University of Technology）的研究团队提出了深度学习方法 DLKcat来预测所有代谢酶与其底物的 kcat值，只需要底物 SMILES 信息和酶的蛋白质序列作为输入，从而为任何物种产生通用的 kcat预测工具。文章中使用pearson系数对模型进行评估（代表预测值与实际值之间的相关性系数），表现还不错，但是在实际测试中该方法预测出来的kcat值误差非常大。\n",
    "\n",
    "为此，我们希望通过建立PBkcat模型，该模型使用proteinBERT提取蛋白质氨基酸序列特征、使用GNN提取底物分子图特征，再使用attention机制对kcat值进行大规模预测，希望能进一步探究蛋白质和底物分子与kcat值之间的联系，能够更加准确的预测kcat值。\n",
    "\n",
    "## 相关算法介绍：\n",
    "\n",
    "### ProteinBERT\n",
    "2018年，Devlin等提出基于深度双向Transformer的预训练模型ProteinBERT，旨在以一种自然的方式捕获蛋白质的局部和全局表示。ProteinBERT在涵盖不同蛋白质属性（包括蛋白质结构、翻译后修饰和生物物理属性）的多个基准上获得了最先进的性能。\n",
    "\n",
    "### Substructure-based graph neural network (sub-GNN)\n",
    "对于底物的处理，我们打算通过GNN来提取底物的分子图特征。分子图以原子为节点，键为边的图形表示，节点存储信息（标签），例如原子类型、电荷、多重性和质量，而边存储键合顺序。每个都可以具有关于芳族和立体异构的信息。\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title clone and download dependencies\n",
    "! git clone https://github.com/950288/PBkcat_test.git\n",
    "%cd PBkcat_test\n",
    "! pip install  -r dependencies.txt\n",
    "! wget -nc -P preprocess ftp://ftp.cs.huji.ac.il/users/nadavb/protein_bert/epoch_92400_sample_23500000.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title convert substrate smiles to fingerprints for training\n",
    "# ! python preprocess/substrate.py\n",
    "\n",
    "import json\n",
    "from rdkit import Chem\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "\n",
    "radius = 2\n",
    "ngram = 3\n",
    "\n",
    "def create_atoms(mol):\n",
    "    \"\"\"Create a list of atom (e.g., hydrogen and oxygen) IDs\n",
    "    considering the aromaticity.\"\"\"\n",
    "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    for a in mol.GetAromaticAtoms():\n",
    "        i = a.GetIdx()\n",
    "        atoms[i] = (atoms[i], 'aromatic')\n",
    "    atoms = [atom_dict[a] for a in atoms]\n",
    "    return np.array(atoms)\n",
    "\n",
    "def create_ijbonddict(mol):\n",
    "    \"\"\"Create a dictionary, which each key is a node ID\n",
    "    and each value is the tuples of its neighboring node\n",
    "    and bond (e.g., single and double) IDs.\"\"\"\n",
    "    # bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "    return i_jbond_dict\n",
    "\n",
    "\n",
    "\n",
    "def extract_fingerprints(atoms, i_jbond_dict, radius):\n",
    "    \"\"\"Extract the r-radius subgraphs (i.e., fingerprints)\n",
    "    from a molecular graph using Weisfeiler-Lehman algorithm.\"\"\"\n",
    "\n",
    "    # edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "\n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        fingerprints = [fingerprint_dict[a] for a in atoms]\n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "\n",
    "        for _ in range(radius):\n",
    "            \"\"\"Update each node ID considering its neighboring nodes and edges\n",
    "            (i.e., r-radius subgraphs or fingerprints).\"\"\"\n",
    "            fingerprints = []\n",
    "            for i in range(len(nodes)):\n",
    "                neighbors = i_jedge_dict.get(i, []) \n",
    "                if not neighbors:\n",
    "                    fingerprint = (nodes[i], ())  # empty tuple\n",
    "                else:\n",
    "                    fingerprint = (nodes[i], tuple([(nodes[j], bond) for j, bond in neighbors]))\n",
    "                fingerprints.append(fingerprint_dict[fingerprint])\n",
    "            nodes = fingerprints\n",
    "\n",
    "            \"\"\"Also update each edge ID considering two nodes on its both sides.\"\"\"\n",
    "            _i_jedge_dict = defaultdict(lambda: [])\n",
    "            for i, j_edge in enumerate(i_jedge_dict.values()):\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    _i_jedge_dict[i].append((j, edge))\n",
    "            i_jedge_dict = _i_jedge_dict                 \n",
    "\n",
    "    return np.array(fingerprints)\n",
    "\n",
    "def create_adjacency(mol):\n",
    "    adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "    return adjacency\n",
    "\n",
    "def dump_dictionary(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dict(dictionary), file)\n",
    "\n",
    "def save_array(array, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(array, file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open('./data/Kcat_combination_0918.json', 'r') as infile :\n",
    "        Kcat_data = json.load(infile)\n",
    "        \n",
    "    compound_fingerprints = list()\n",
    "    adjacencies = list()\n",
    "    Kcats = list()\n",
    "\n",
    "    for data in tqdm.tqdm(Kcat_data) :\n",
    "        smiles = data['Smiles']\n",
    "        Kcats.append(float(data['Value']))\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles)) # Add hydrogens\n",
    "        atoms = create_atoms(mol) # Get atom features\n",
    "\n",
    "        i_jbond_dict = create_ijbonddict(mol) # Get graph structure\n",
    "        fingerprints = extract_fingerprints(atoms, i_jbond_dict, radius) # Extract fingerprints\n",
    "        compound_fingerprints.append(fingerprints)\n",
    "\n",
    "        adjacency = create_adjacency(mol)\n",
    "        adjacencies.append(adjacency)\n",
    "\n",
    "    save_array(Kcats, './data/Kcats.pickle')\n",
    "    save_array(compound_fingerprints, './data/compound_fingerprints.pickle')\n",
    "    save_array(adjacencies, './data/adjacencies.pickle')\n",
    "    dump_dictionary(atom_dict, './data/atom_dict.pickle')\n",
    "    dump_dictionary(bond_dict, './data/bond_dict.pickle')\n",
    "    dump_dictionary(fingerprint_dict, './data/fingerprint_dict.pickle')\n",
    "    dump_dictionary(edge_dict, './data/edge_dict.pickle')\n",
    "\n",
    "    print('compound_fingerprints, adjacencies, atom_dict, bond_dict, fingerprint_dict, edge_dict saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title preprocess protein\n",
    "# ! python preprocess/proteinBERT_local_rep.py\n",
    "\n",
    "from proteinbert import load_pretrained_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model(local_model_dump_dir = \"./preprocess\" , local_model_dump_file_name = 'epoch_92400_sample_23500000.pkl')\n",
    "\n",
    "local_representations = []\n",
    "\n",
    "with open('./data/Kcat_combination_0918.json', 'r') as infile :\n",
    "    Kcat_data = json.load(infile)\n",
    "\n",
    "def dump_dictionary(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dict(dictionary), file)\n",
    "\n",
    "def save_array(array, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(array, file)\n",
    "\n",
    "sequences = []\n",
    "max_len = 0\n",
    "for i , data in enumerate(Kcat_data) :\n",
    "    sequences.append(str(data['Sequence']))\n",
    "    max_len = max(max_len , len(data['Sequence']))\n",
    "\n",
    "max_len += 2\n",
    "\n",
    "model = pretrained_model_generator.create_model(max_len)\n",
    "\n",
    "step = 256\n",
    "for i in range(0, len(sequences), step):\n",
    "    print(i, '/' , len(sequences))\n",
    "    sequences_ = sequences[i:i+step]\n",
    "    input_ids = input_encoder.encode_X(sequences_, max_len)\n",
    "    local_representations_, _ = model.predict(input_ids, batch_size=16)\n",
    "    if len(local_representations) != 0:\n",
    "        local_representations = np.concatenate((local_representations, local_representations_), axis=0)\n",
    "    else:\n",
    "        local_representations = local_representations_\n",
    "\n",
    "print(local_representations.shape)\n",
    "save_array(local_representations, './data/local_representations.pickle')\n",
    "# save_array(local_representations, './data/local_representations.pickle')\n",
    "\n",
    "print('saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title train model\n",
    "import model.model as model\n",
    "import torch\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_name = 'Kcat'\n",
    "\n",
    "    args = {\n",
    "        \"dim\" : 10,\n",
    "        \"layer_output\" : 3,\n",
    "        \"layer_gnn\" : 3,\n",
    "        \"layer_dnn\" : 3,\n",
    "        \"lr\" : 1e-3,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"epoch\" : 100\n",
    "    }\n",
    "\n",
    "    file_model = './model/output/' + model_name\n",
    "    file_MAEs  = './model/output/' + model_name + '-MAEs.csv'\n",
    "    file_args  = './model/output/' + model_name + '-args.json'\n",
    "\n",
    "    dir_input = './data/'\n",
    "    compound_fingerprints = model.load_pickle(dir_input + 'compound_fingerprints.pickle')\n",
    "    adjacencies = model.load_pickle(dir_input + 'adjacencies.pickle')\n",
    "    proteins_local = model.load_pickle(dir_input + 'local_representations.pickle')\n",
    "    # proteins_global = model.load_pickle(dir_input + 'global_representations.pickle')\n",
    "    fingerprint_dict = model.load_pickle(dir_input + 'fingerprint_dict.pickle')\n",
    "    args['len_fingerprint'] = len(fingerprint_dict)\n",
    "    Kcat = model.load_pickle(dir_input + 'Kcats.pickle')\n",
    "    Kcat = torch.LongTensor(Kcat)\n",
    "\n",
    "    if not (len(compound_fingerprints) == len(adjacencies) == len(proteins_local) == len(Kcat)):\n",
    "        print('The length of compound_fingerprints, adjacencies and proteins are not equal !!!')\n",
    "        exit()\n",
    "\n",
    "    dataset = list(zip(compound_fingerprints, adjacencies, proteins_local, Kcat))\n",
    "    random.shuffle(dataset)\n",
    "    dataset_train, dataset_ = model.split_dataset(dataset, 0.8)\n",
    "    dataset_dev, dataset_test = model.split_dataset(dataset_, 0.5)\n",
    "\n",
    "    \"\"\"CPU or GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses GPU !!!')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU !!!')\n",
    "\n",
    "    torch.manual_seed(random.randint(1, 10000))\n",
    "    Kcatpredictor = model.KcatPrediction(args, device).to(device)\n",
    "    trainer = model.Trainer(Kcatpredictor)\n",
    "    tester = model.Tester(Kcatpredictor)\n",
    "\n",
    "    \"\"\"Output files.\"\"\"\n",
    "    with open(file_args, 'w') as f:\n",
    "        f.write(str(json.dumps(args)) + '\\n')\n",
    "\n",
    "    \"\"\"Start training.\"\"\"\n",
    "    print('Training...')\n",
    "    MAEs = []\n",
    "    start = timeit.default_timer()\n",
    "    for epoch in range(0, args[\"epoch\"]):\n",
    "        print('Epoch: %d / %d' % (epoch + 1, args[\"epoch\"]))\n",
    "        LOSS_train, RMSE_train, R2_train = trainer.train(dataset_train)\n",
    "        LOSS_test, RMSE_test, R2_test = tester.test(dataset_dev)\n",
    "        end = timeit.default_timer()\n",
    "        time = end - start\n",
    "        MAE = [epoch+1, time, LOSS_train, RMSE_train, R2_train, \n",
    "                            LOSS_test,  RMSE_test,  R2_test]\n",
    "        MAEs.append(MAE)\n",
    "\n",
    "    \"\"\"Save the trained model.\"\"\"\n",
    "    torch.save(Kcatpredictor.state_dict(), file_model + \".pth\")\n",
    "    print('Model saved to %s' % file_model)\n",
    "\n",
    "    \"\"\"save MAEs as csv file\"\"\"\n",
    "    with open(file_MAEs, 'w') as f:\n",
    "        f.write('epoch, time, LOSS_train, RMSE_train, R2_train, LOSS_test, RMSE_test, R2_test\\n')\n",
    "        for MAE in MAEs:\n",
    "            f.write(str(MAE)[1:-1] + '\\n')\n",
    "    print('MAEs saved to %s' % file_MAEs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
